#The C10K Problem

[原文](http://www.kegel.com/c10k.html)

原作 Dan Kegel  翻譯 Yang Acer

你不覺得現在是網頁伺服器得同時應付上萬(10K)個客戶端的時代嗎!? 畢竟，網際網路現在已經是個大雜院了!

而電腦也越來越強大；你可以花不到新台幣兩萬塊，購買配備有 2GHz 處理器、2GB 記憶體、1Gbps 網卡(*)的設備。咱們來算算─兩萬個客戶端，每個可以分配到 100KHz 運算能力, 100KB 記憶體與  50Kbps (*)；這樣看來，讀取 4K 的資料並傳送給這兩萬個用戶應該不用花到一秒鐘(另外一提，這樣的動作花費是一塊錢，那些每個客戶要花100鎂才能用的作業系統看起來是不是有點，痾...重量級!)。總之，現在硬體再也不是瓶頸了。

1999 年，最繁忙的 FTP 站 cdrom.com 已經透過 Gigabit Ethernet 管道同時服務一萬個客戶。2001 年時，數個對此市場抱有期待的電信業者已經開始提供相同的速度(譯註：台灣落後幾年要不要算一下?)

而簡易型客戶端的運算模式開始復活─不同的是，在網際網路上你就能找到可以服務上千個客戶的伺服器。

當你體認了上述的情境，這篇文章提供一些如何調教調教、撰寫程式來支援上萬個客戶端。既然我主要的興趣是 UNIX ，主要的討論是集中在 UNIX 類的系統，不過 Windows 也聊備一格。

##內容

- The C10K problem

- 相關網站

- 必備讀物

- I/O 框架

- 輸出/入策略
  
  1. 單緒非阻塞式 I/O  與階段觸發式可讀取通知

    - 傳統的 select()
    
    - 傳統的 poll()
    
    - /dev/pool (Solaris 2.7+)
    
    - kqueue (FreeBSD, NetBSD)
  
  2. 單緒非阻塞式 I/O 與邊緣觸發式可讀取通知
  
    - epoll (Linux 2.6+)
    
    - Polyakov's kevent (Linux 2.6+)
    
    - Drepper's New Network Interface (Lunux 2.6+ 提議中)
    
    - 即時訊號 (Linux 2.4+)
    
    - Signal-per-fd
    
    - kqueue (FreeBSD, NetBSD)
    
  3. 單緒非同步 I/O 與完成通知
  
  4. 多緒
    
    - LinuxThreads (Linux 2.0+)
    
    - NGPT (Linux 2.4+) 

    - NPTL (Linux 2.6, Red Hat 9) 

    - FreeBSD 多緒支援

    - NetBSD 多緒支援
    
    - Solaris 多緒支援

    - Java 多緒支援 (JDK 1.3.x and earlier)
    
    - 1:1 threading vs. M:N threading
    
  5. 系統核心內建網頁伺服器
  
- 評語

- 檔案代碼的限制

- 多緒的限制

- Java 的議題

- 其他訣竅

  - 零拷貝
  
  - sendfile() 系統函式可用來時做零拷貝網路傳輸
  
  - 使用 writev 或 TCP_CORK 避免零碎分頁
  
  - 有些程式可受益於非 POSIX 執行緒
  
  - 快取自己的資料有時很划算
  
- 其他限制

- 系統核心議題

- 測量伺服器效能

- 有趣的範例

  - select() 伺服器
  
  - /dev/poll 伺服器
  
  - kqueue() 伺服器
  
  - 即時訊號 伺服器
  
  - 多緒 伺服器
  
  - 核心內建式 伺服器
  
- 其他有趣的連結

<!--- Content delimiter -->

##相關網站

2003 年 10 月，Felix von Leitner 以網頁與投影片的方式發表針對網路規模(network scalability)的效能評測，完整地涵蓋了各種網路系統與作業系統。他所觀察到的其中一點：Linux 2.6 核心完全地擊敗了 2.4 核心，不過，他提供的圖表讓作業系統開發者多了許多考量因素(Slashdot 的評語也可以看看；如果有任何人更新 Felix 的效能評鑑，也是讓人很感興趣的)。

##必備讀物

如果你還沒讀過 W. Richard Stevens 寫的 UNIX Network Programming: Networking APIs: Sockets and Xti (Volume 1)，快去買一本吧！他描述了許多撰寫高效能伺服器的 I/O 策略，甚至也討論到 thundering herd 問題。還有，既然你已經在看這篇文章，也去讀讀 Jeff Darcy notes on high-performance server design。

(對於網頁伺服器的使用者而非開發者來說，或許可以讀一下 Cal Henderson 所著的 Building Scalable Web Sites。)

##I/O 框架

下面所列的函示庫已經打包了一些後文將提及的技術，它們能讓開發者從系統相依性中解放出來並提高程式碼有更好的可攜性。

* [ACE](http://www.cs.wustl.edu/~schmidt/ACE.html) 重量級的 C++ 框架，以物件導向實作數種 I/O 策略；特別一提，ACE 提出的 Reactor 範式(*) 是物件導向式的非阻塞式 I/O 而 Proactor 範式(*) 是物件導向式的非同步 I/O。

* [asio](http://asio.sf.net/) 已被納入 Boost 函示庫，有點類似標準樣板函示庫版的 ACE (譯註：asio 提供比較多與作業系統相依的特化，比如 kqueue 在 ACE 中並沒有預設實作，而 ACE 則提供了非常完整的 Windows 版 POSIX API，雖然這樣比喻不倫不累，不過 ACE 幾乎可以取代 MinGW)。

* [libevent](http://monkey.org/~provos/libevent) Niels Provos 所寫的輕巧的 C I/O 框架。支援 kqueue 、 select()、poll() 與 epoll(*)。他只提供階段式觸發，我認為這有好有壞。Niels 提供了一個不錯的圖表，呈現當連線數變化時，處理事件的時間會如何增加(*)，說明了 kqueue 與 sys_epoll 是無疑的贏家。

* 我自己寫的輕量框架(蘇郎傷心，已經沒在更新了)

  * Poller 是輕量的 C++ I/O 框架，實作階段式觸發的上層 API，包裝底層 API 如 poll、 select、 /dev/poll、 kqueue、 或是 sigio(*)。他用來作底層 API 的效能評測還不錯用。這份文件後面提供的 Poller 連結描繪了如何使用各別的底層 API 。
  
  * rn 是個輕量的 C I/O 框架，是 Poller 之後的試作，以 LGPL 授權，也因此較適合商業使用，以 C 實作，讓一些非 C++ 應用程式可以使用。某些商業產品中有用到這個框架。
  
* Matt Welsh 在 2000 年四月發表了一篇文章，討論撰寫可擴展伺服器時，如何平衡工作執行緒與事件驅動技術的文章。該文章有提及他的 Sandstorm I/O 框架。

* Cory Nelson 的 Scale! - Windows 下的非同步 socket, 檔案與管線 I/O 函示庫。

##I/O 策略

網路程式的設計者有許多選擇，這裡是一小部分：

- 單緒是否需要一次進行多個 I/O 呼叫，又如何達到

  - 不需要：在多緒或多行程下使用阻塞型或同步呼叫即可。
  
  - 使用非阻塞式呼叫 (例如對設定為 O_NONBLOCK 的 socket 呼叫 write()) 啟動 I/O 與讀取通知 (例如 poll() 或 /dev/poll) 來獲知何時可以進行下一個 I/O 。通常來說只有網路 I/O 適用，對硬碟 I/O 不適用。
  
  - 使用非同步呼叫(例如：aio_write()) 來啟動 I/O，倚靠完成通知(例如：訊號或完成埠) 來獲知何時 I/O 完成。對網路或硬碟 I/O 都適用。

- 如何控制服務各個客戶端的邏輯(*)

  - 一個行程對一個客戶端 (經典的 UNIX 方式，從 1980 左右就開始使用了)
  
  - 一個核心緒處理多個客戶端；一個客戶端可由以下控制：
  
    - 一個非核心緒 (如 GNU 狀態緒, Java w/green thread)
    
    - 一個狀態機
    
    - a continuation

  - 一個核心緒對一個客戶端 (如 Java 中的原生執行緒)
  
  - 一個核心緒對一個動作中的客戶端 ( 如以 Apache 為前端的 Tomcat, Windows NT 的完成埠及執行緒池)

- 使用標準的作業系統服務或直接把邏輯放到核心 (如客製化驅動，核心模組等)

以下五個組合似乎還蠻流行的：

  1. 每個執行緒服務多個客戶端，使用非阻塞式 I/O 與階段觸發式讀取通知
  
  2. 每個執行緒服務多個客戶端，使用非阻塞式 I/O 與邊緣觸發式讀取通知
  
  3. 每個執行緒服務多個客戶端，使用非同步 I/O
  
  4. 每個執行緒服務一個客戶端，使用阻塞式 I/O
  
  5. 核心內建式伺服器


###1. 非阻塞式 I/O 與階段觸發式可讀取通知

... 將所有網路代碼設定為非阻塞模式，並使用 select() 或 poll() 來分辨那一個代碼有資料在等待中，是一種傳統愛用的方式。核心會通知你一個檔案代碼已經就緒，不論自上一次的通知後你對該代碼進行的動作是否已經完成(階段式觸發這個名詞來自於電腦硬體設計，它相對於邊緣觸發。Jonathon Lemon 在 BSDCON 2000 介紹 kqueue() 的文件中使用了這個名詞)。

註：記得！來自於核心的讀取通知只是個提示，這非常重要；當你要讀取時，一個檔案代碼可能已不在就緒狀態。這也是為何，非得要在非阻塞模式下使用讀取通知。

這個方式中有個重要的瓶頸：當 read() 或 sendfile() 從硬碟讀取而該分頁不在記憶體中；由於非阻塞模式對硬碟中的檔案代碼沒有作用(譯註：因此讀取本機檔案會造成阻塞)。同樣的狀況也會發生在記憶體映照式檔案。當一個伺服器第一次需要硬碟操作，它的行程會被阻塞，所有客戶端必須等待該操作完成，而原始的線性效能就被浪費掉了。這就是非同步 I/O 想要解決的，不過對於缺乏 AIO 的系統來說，使用額外的執行緒或者行程來處理硬碟操作也能解決這個瓶頸。其中一種方式是用記憶體映照檔案，如果 mincore() 指出有 I/O 需求，就讓一個工作行程(或執行緒)去處理該需求，然後繼續處理網路端的流量。Jef Poskanzer 曾提及 Pai、Druschel、及 Zwaenepoel 在 1999 年發表的 Flash 網路伺服器使用了該技巧；他們在 Usenix '99 給了一個相關的演講。看起來，mincore() 多可見於 BSD 族系如 FreeBSD 及 Soloaris，不過並不在 Single Unix Speciication 中。Linux 核心版本 2.3.51 也有這個函式，這得感謝 Chuck Lever。

不過，在2003 年 11 月，Vivek Pei 等人在 freebsd-hackers list 發表了一個不錯的結果，內容關於他們以系統剖析的方式，找出並攻克 Flash 網頁伺服器的各項瓶頸。其中一項瓶頸就是 mincore() (看來他終究不是個好主意)。另一個就是 sendfile() 會被阻塞在硬碟操作；他們修改出新版的 sendfile() ，該函式在硬碟分頁不在記憶體時，會回傳 EWOULDBLOCK 一類的傳回值，藉以減少阻塞。(不太確定要如何告訴使用者該分頁已經存在記憶體，在我看來真正需要的是 aio_sendfile()。) 最後，他們最佳化的結果─在 1GHz/1GB FreeBSD 機器上的 SpecWeb99 拿到 800 的分數，是 spec.org 上，檔案處理服務類型裡最好的分數。

有數種方法在單緒中得知一組非阻塞式 sockets 已經就緒：

  * 傳統的 select()
  
    很不幸，select() 可監看的代碼數量受限於系統的 FD_SETSIZE，而它在標準函示庫常數及使用者的程式中都是編譯期常數(有些版本的 C 函示庫允許編譯使用者程式時重定義該常數以取得更大數量)。  可參考 Poller_select(cc, h) 這個例子，展示如何交替使用 select() 與其他讀取通知機制。
    
  * 傳統的 poll()  
  
    對於代碼數量，poll() 沒有硬性的限制，不過在數量超過數千個之後，掃描過所有代碼還是會花不少時間，即使大部分的代碼都處於閒置狀態。有些作業系統(如 Solaris 8) 使用提取提示來加速 poll()，Linux 上的實作與效能評測是由 Niels Provos 在 1999 年完成。

    見 Poller_poll(cc, h, benchmarks) 這個例子。
  
  * /dev/poll
  
    在 Solaris 中，這是個推薦的 poll() 替代方案。

    /dev/poll 背後的概念是利用我們其實會以相同的參數，呼叫很多次的 poll()。有 /dev/poll 時，你可以開啟該裝置並取得(譯註：代表/dev/poll 的)代碼，然後藉由寫入該代碼來通知作業系統你有興趣的檔案；之後，你只需要從該代碼就能讀取到目前已就緒的檔案代碼。

    這個功能悄悄出現在 Solaris 7 (請見 [patchid 106541](http://sunsolve.sun.com/pub-cgi/retrieve.pl?patchid=106541&collection=fpatches))，在 Solaris 8 時對外公開；根據 Sun 的說法， 750 個客戶時，它的負擔只有 poll() 的 10% 。

    Linux 上有許多 /dev/poll 的試作品，不過沒有一個比得上 epoll()，目前也沒有真正完整的實作。 /dev/poll 在 Linux 上是不推薦使用。

    見 Poller_devpoll(cc, h, benchmarks) 這個例子。警告─該範例是針對 Linux /dev/poll，不一定能在 Solaris 下運作！
  
  * kqueue()
  
    在 FreeBSD 與 NetBSD 中，這是個推薦的 poll() 替代方案。

    請見後文，kequeue() 邊緣觸發與階段觸發都有支援。
  
###2. 非阻塞式 I/O 與邊緣觸發式可讀取通知

邊緣式觸發意指：你給核心一個檔案代碼，當該代碼從__非就緒__變成__就緒__時，核心會通知你。接著核心會假設你已經知道該帶碼為就緒，而不會再送出任何通知，直到你對該檔案代碼作了一些操作並造成該代碼不再為就緒狀態(例如，直到你呼叫 send(), recv() 或 accept() 時收到 EWOULDBLOCK 錯誤─該錯誤在 send/recv 無法滿足指定的位元組長度時也會生)。

當你使用邊緣式觸發時，你必須預期有偽事件，因為常見的實作只要在收到封包時就會送出通知，不會管代碼是否就緒。

而相對於階段式觸發，由於邊緣式觸發只會通知一次，他較不能容忍設計失誤，像是一個連線會因為一個不小心忽略的事件而被永遠卡死。話雖如此，我發現邊緣式觸發讀取通知，在設計使用 OpenSSL 的非阻塞式客戶端時，還蠻容易的，值得試試。

- epoll

  Linux 2.6 核心下推薦使用的邊緣式觸發 poll 替代方案。

  2001 年 7 月 11 日，Davide Libenzi 提出了一個更新作為即時訊號的新方案，現稱 [/dev/epoll](http://www.xmailserver.org/linux-patches/nio-improve.html)。跟即時訊號式讀取通知頗像，但他合併了重複的事件，並提供更有效率的大批事件擷取機制。

  在 epoll 介面由 /dev 更改為系統函式 sys_epoll 後，被併入了 2.5 版核心，確切版號為 2.5.46。而 2.4 版核心則有舊版 epoll 介面的更新檔可用。

  2002 萬聖節時，有一長串關於[統合 epoll, aio 與其他事件來源](http://marc.theaimsgroup.com/?l=linux-kernel&m=103607925020720&w=2)的討論。這事可能還沒發生，不過 Davide 正專注於強化 epoll 的泛用性。

- Polyakov's kevent (Linux 2.6+) 

  2006 年 2 月 9 日及 7 月 9 日，Evgeniy Polykov 提出了看似統合 epoll 與 aio 的更新；他的目標在於支援網路 AIO，請見

  * [the LWN article about kevent](http://lwn.net/Articles/172844/)

  * [his July announcement](http://lkml.org/lkml/2006/7/9/82)

  * [his kevent page](http://tservice.net.ru/~s0mbre/old/?section=projects&item=kevent)

  * [his naio page](http://tservice.net.ru/~s0mbre/old/?section=projects&item=naio)

  * [some recent discussion](http://thread.gmane.org/gmane.linux.network/37595/focus=37673)

- Drepper's New Network Interface (Lunux 2.6+ 提議中)

  在 OLS 2006 時，Ulrich Drepper 提出了一個新的高速非同步網路 API。

  請見

  * 他的文章，"[The Need for Asynchronous, Zero-Copy Network I/O](http://people.redhat.com/drepper/newni.pdf)"

  * [他的投影片](http://people.redhat.com/drepper/newni-slides.pdf)

  * [LWN 在 7 月 22 日的文章](http://lwn.net/Articles/192410/)


- 即時訊號

  推薦在 Linux 2.4 核心作為邊緣是觸發 poll 的替代方案。

  該核心可以藉由特化的即時訊號導送 socket 可讀取事件。以下是啟動該行為的方式：


  ```C
      /* Mask off SIGIO and the signal you want to use. */
      sigemptyset(&sigset);
      sigaddset(&sigset, signum);
      sigaddset(&sigset, SIGIO);
      sigprocmask(SIG_BLOCK, &m_sigset, NULL);
      /* For each file descriptor, invoke F_SETOWN, F_SETSIG, and set O_ASYNC. */
      fcntl(fd, F_SETOWN, (int) getpid());
      fcntl(fd, F_SETSIG, signum);
      flags = fcntl(fd, F_GETFL);
      flags |= O_NONBLOCK|O_ASYNC;
      fcntl(fd, F_SETFL, flags);
  ```
      
  這段程式碼會在普通的 I/O 函式如 read()、write() 完成後送出訊號。要使用這段程式，見以下虛擬碼及 [sigwaitinfo()](http://www.opengroup.org/onlinepubs/007908799/xsh/sigwaitinfo.html)：

  ```C
      while(poll(...)){
        handle_notified_fd();
        // 上面提到的程式碼
        sigwaitinfo();
      }
  ```

  如果 sigwaitinfo 或 sigtimedwait 回傳了一個即時訊號，siginfo.si_fd 與 siginfo.si_band ，它們提供的資訊和呼叫 poll() 後的 poollfd.fd 、 pollfd.revents 幾乎相同；接著你就能處理該 I/O 在繼續呼叫 sigwaitinfo()。

  若 sigwaitinfo 回傳傳統的 SIGIO，表示訊號佇列已經滿了，所以你得藉由暫時改變 SIG_DFL 的訊號處理器來清空訊號佇列，然後中斷 sigwaitinfo() 回到外層的 poll() 迴圈起點。

  實例請參見 Poller_sigio(cc, h) 。

  [Zach Brown's phhttpd](http://www.kegel.com/c10k.html#phhttpd) 有一個直接使用該功能的例子。(或者別看，他有點難搞懂...)

- Signal-per-fd

  Chandra 與 Mosberger 提出了一個即時訊號的改良版，稱為 signal-per-fd ，利用合併重複事件的方式，可以減少或免除即時訊號佇列溢位；不過它並未超越 epoll 。[他們的文章](http://www.hpl.hp.com/techreports/2000/HPL-2000-174.html) 有比較他們的機制與 select()、/dev/poll 的效能。
  
  Vitaly Luban 在 2001 年 5 月 18 日宣布了一個該機制的[更新](http://www.luban.org/GPL/gpl.html) (註：直到 2001, 9 月時，在工作量大狀況下，該更新仍有一些穩定性問題，dkftpbench 在 4500 用戶時有可能觸發這個狀況。)
  
  實例請見 Poller_sigfd(cc, h)。

- kqueue() 

  FreeBSD 與 NetBSD 平台上推薦的邊緣式觸發 poll 替代方案。
  
  FreeBSD 4.3 之後的版本與 NetBD 2002 10 月後的版本，支援一個泛化的 poll() 替代品，稱為 [kqueue()/kevent()](http://www.freebsd.org/cgi/man.cgi?query=kqueue&apropos=0&sektion=0&manpath=FreeBSD+5.0-current&format=html)；它同時支援邊緣式與階段式觸發。(參閱[Jonathan Lemon 的網頁](http://people.freebsd.org/~jlemon/)與他的文章 [kqueue(), BSDCon 2000](http://people.freebsd.org/~jlemon/papers/kqueue.pdf) )

  類似 /dev/poll，你一樣得配置一個監聽物件，不過不是透過開啟 /dev/poll 這個檔案，而是呼叫 kqueue() 來獲得。要改變你監聽的事件，或取得目前的事件列表，需呼叫 kevent() 並將之前透過 kqueue() 得到的監聽物件作為參數。kqueue() 不只能監聽 socket，對一般的檔案、訊號，甚至是 I/O 完成事件都能監聽。
  
  __註__：2000 年 10 月左右，FreeBSD 上的執行緒函示庫不太能跟 kqueue() 互動，像是 kqueue() 卡住時，整個行程都會卡死，而不是只有呼叫的執行緒卡死。
  
  kqueue() 範例請參閱 Poller_kqueue(cc, h, benchmarks)。
  
  使用 kqueue() 的範例與函示庫：
  
  * [PyKQueue](http://people.freebsd.org/~dwhite/PyKQueue/) - Python 的 kqueue() 連結。
  
  * [Ronald F. Guilmette 的 echo 伺服器範例](http://www.monkeys.com/kqueue/echo.c)；另請見在 2000 年九月他發在 [freebsd questions 的文章](http://groups.yahoo.com/group/freebsd-questions/message/223580)。
  
###3. 非同步 I/O 與完成通知

###4. 多緒
    
    - LinuxThreads (Linux 2.0+)
    
    - NGPT (Linux 2.4+) 

    - NPTL (Linux 2.6, Red Hat 9) 

    - FreeBSD 多緒支援

    - NetBSD 多緒支援
    
    - Solaris 多緒支援

    - Java 多緒支援 (JDK 1.3.x and earlier)
    
    - 1:1 threading vs. M:N threading
    
  5. 系統核心內建網頁伺服器
  
- 評語

- 檔案代碼的限制

- 多緒的限制

- Java 的議題

- 其他訣竅

  - 零拷貝
  
  - sendfile() 系統函式可用來時做零拷貝網路傳輸
  
  - 使用 writev 或 TCP_CORK 避免零碎分頁
  
  - 有些程式可受益於非 POSIX 執行緒
  
  - 快取自己的資料有時很划算
  
- 其他限制

- 系統核心議題

- 測量伺服器效能

- 有趣的範例

  - select() 伺服器
  
  - /dev/poll 伺服器
  
  - kqueue() 伺服器
  
  - 即時訊號 伺服器
  
  - 多緒 伺服器
  
  - 核心內建式 伺服器
  
- 其他有趣的連結

中英對照表

	非同步  async, asynchronous
	非阻塞  non-blocking
	緒, 執行緒    thread
	階段觸發式  level-triggered
	邊緣觸發式  edge-triggered
	效能評測  benchmark
	狀態機  state machine
	完成埠  completion port
	執行緒池  thread pool
	管線  pipe
	代碼  handle, descriptor
	核心  kernel
	記憶體映照檔案  memory-mapped file
	非同步 I/O	AIO
	訊號佇列	signal queue
	訊號處理器	signal handler
